{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Brief Intro to Tensorflow (TF)\n",
    "For this tutorial, we'll get started by talking a little bit about how TF works and then build a small demo classification using TF to run on CPU. My goal is for everyone to be able to go home and implement their own TF models, or at least be able to follow what some source code means for more advanced applications. I'll present a demo of logistic regression in TF, since logistic regression is a great foundation for learning about more advanced neural network architectures. You can think of logistic regression as the fundamental building block of deep learning networks. \n",
    "\n",
    "For a TF tutorial straight from the source, you can go to: https://www.tensorflow.org/tutorials/wide It goes more in-depth about how to select and combine features within tensorflow, whereas I did most of that work in pandas, because it's easier and more straightforward. However, if you're building a production system that will learn, update, and repeat, there are some helpful functions for handling unknown datatypes in that tutorial.\n",
    "\n",
    "## Tensors and Graphs\n",
    "At its core, TF is made up of tensors. According to the Wikipedia entry:\n",
    "> In mathematics, tensors are geometric objects that describe linear relations between geometric vectors, scalars, and other tensors. Elementary examples of such relations include the dot product, the cross product, and linear maps.\n",
    "\n",
    "Thanks, Wikipedia. For TF neural networks, the \"data structures\" you mainly need to be concerned about scalars, vectors, matrices, and stacks of matrices. TF encodes these data structures into tensor objects instead of leaving them as scalars, vectors, etc, and it does this because of the graph functionality, which I'll talk about later. If you're familiar with numpy arrays, TF works with those, so that's not a problem. The mathematical functions you mainly need to know are the dot product and linear maps (linear transformations) for manipulating the input and transformed data structures.\n",
    "\n",
    "## Graphs?\n",
    "Yea, buddy. TF is setup so that the *tensors* **flow** through *graphs*. Get it? So, going back to the tensors I just introduced, you would input your data into a TF tensor object like so,  `A = tf.constant(1234) `, and then to display that constant, you would have to run it through a TF *session*, like so:\n",
    "```\n",
    "A = tf.constant(1234)\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(A)\n",
    "    print(output)\n",
    "```\n",
    "\n",
    "Why do we need the session? From the documentation:\n",
    "> A Session object encapsulates the environment in which Operation objects are executed, and Tensor objects are evaluated.\n",
    "\n",
    "Basically, it's accounting for the data that you have, how you will input that data (usually in a pipeline), creating the operations on the data, and executing, all before you have even imported the data. This gives flexibility in how you import data, whether in batches (as is typical with data >1Gb e.g. images, tons of pdfs, etc.) or all at once, and how to allocate where operations take place, whether on a CPU or a GPU. This flexibility is exactly what AlphaGo used to create a policy and value networks, using deep learning (on GPU) and policies from tree decisions (CPU). The following image gives a better representation of the flexibility allowed by using the session function.\n",
    "![Image of Logistic Regression](https://image.slidesharecdn.com/k2jeffdean-160609173832/95/large-scale-deep-learning-with-tensorflow-31-638.jpg?cb=1465493958)\n",
    "\n",
    "The rest of this tutorial will be focused on using a local machine with a reasonable amount of data and performing calculations on the CPU. \n",
    "\n",
    "Let's get started by building a logistic regression with some example data. The goal will be to predict people who will default on their next credit card payment.  You can download the data from this UCI repository: https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients\n",
    "\n",
    "Also, here is an example of what the output of a finalized tensorflow model would look like on the Tensorboard. This is where we want to get to.\n",
    "![Image of Logistic Regression](https://www.tensorflow.org/images/tensors_flowing.gif)\n",
    "\n",
    "One of the nuances that drive people crazy about TF is how you input data. There are multiple methods for inputting your data, each of which depends on where your data is coming from. The TF Dataset API has much more capabilities for importing and using data, but for this first example I'll stick with the basics.\n",
    "\n",
    "For this tutorial, we'll build a Logistic Regression classifier from the ground up to determine whether someone from the dataset will default on their next credit card payment. Logisitic regression (LogR) classifiers are the 'lego blocks' for Neural Networks, so an understanding of LogR will help with an understanding of the whole. To perform LogR, we need to basically model:\n",
    "$$sigmoid(W*x + b)$$\n",
    "\n",
    "This sigmoid function is the result of multiplying the inputs by the randomly initialized weights of the system and adding the bias, like so:\n",
    "\n",
    "![Image of Logistic Regression](https://i.stack.imgur.com/bA57S.png)\n",
    "\n",
    "The 'Intelligent' part of the machine learning comes when we define the 'cost function' for our model. Basically, for each data point, or in this case, customer of the credit card company, we will use the cost function to find out how different the calculated output is from the actual output. Gradient descent is where the learning will be happening, or how we change the weights (W) in `W*x+b`. Once the training by gradient descent is done, we'll test it on some hold-out data. We do this to avoid creating a model that's really good at detecting what it already knows.\n",
    "\n",
    "As a recap, we are doing the following in TF:\n",
    "1. Defining a model to use (Logistic Regression)\n",
    "2. Training it (changing weights by gradient descent from output of cost function)\n",
    "3. Testing it against hold-out data\n",
    "\n",
    "As with all ML, we need to start with preprocessing. TF is great for advanced deep learning but is not the tool to assess and clean data. Instead, I'll use pandas to assess, clean, and setup the data for conversion into a TF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Necessary (for the most part) libraries\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# use whatever file name you saved the csv as in the pandas import statement below\n",
    "defaults_df = pd.DataFrame.from_csv('default_of_credit_card_clients.csv')\n",
    "\n",
    "# A little bit of pre-processing\n",
    "new_header = defaults_df.iloc[0]\n",
    "defaults_df = defaults_df[1:]\n",
    "defaults_df.rename(columns = new_header)\n",
    "\n",
    "# Renaming columns and adding target variable\n",
    "new_header = list(new_header)\n",
    "new_header[-1] = 'Y'\n",
    "defaults_df.columns = new_header\n",
    "\n",
    "# Changing to numeric - data was imported as string objects\n",
    "defaults_df = defaults_df.apply(pd.to_numeric)\n",
    "\n",
    "# Removing invalid education values\n",
    "defaults_df = defaults_df[defaults_df.EDUCATION != 0]\n",
    "\n",
    "# One-hot encoding\n",
    "clf_df = pd.get_dummies(defaults_df, columns=[\"SEX\", \"EDUCATION\", \"MARRIAGE\", \"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\"],\n",
    "               prefix=[\"SEX\", \"EDU\", \"MARRY\", \"PAY_0\", \"PAY_2\", \"PAY_3\", \"PAY_4\", \"PAY_5\", \"PAY_6\"])\n",
    "\n",
    "# Setting X (input) and Y(target) variables\n",
    "X = clf_df.drop('Y', axis=1)\n",
    "Y = clf_df.Y\n",
    "Y = Y.values.reshape(29986,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing can be done in pandas, but if you're dealing with something like images, you'd want to make some functions to **normalize** and, potentially, **flatten** your pixel values. Flatten is mainly used for image processing with CNNs. Potentially later, we'll look at an example of preprocessing images and using a CNN (very, very, rough overview).\n",
    "\n",
    "For this model, we now need to convert the data from a pandas dataframe to tensorflow objects. Since this is a small dataset that can fit in memory, we'll just import it as a constant.\n",
    "\n",
    "Let's get to setting this thing up and visualized in Tensorboard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clear Tensorboard graph for troubleshooting\n",
    "tf.reset_default_graph()\n",
    "\n",
    "num_features = len(list(X))\n",
    "num_outputs = 1\n",
    "\n",
    "# We use a placeholder to account for ambiguity of input data type\n",
    "x_input = tf.placeholder(tf.float32, [None, num_features], name = 'inputs')\n",
    "\n",
    "# Now create the variables to be trained\n",
    "W = tf.Variable(tf.truncated_normal([num_features, num_outputs],         # One output and num_features inputs\n",
    "                                    stddev=0.1), name = 'W')   # Truncated normal is a trick that can\n",
    "                                                               # speed up gradient descent\n",
    "b = tf.Variable(tf.zeros([num_outputs]), name = 'bias') # b can be initialized with a zero, but could also use truncated normal\n",
    "\n",
    "# Create sigmoid(W*x + b) where sigmoid(x) = 1 / (1 + exp(-x)). Also called Logits.\n",
    "y_output = tf.sigmoid(tf.matmul(x_input,W) + b, name='sigmoid')\n",
    "\n",
    "# Create the cost Funtion using the real values from Y\n",
    "y_labels = tf.placeholder(tf.float32, [None, num_outputs], name = 'actual_output')\n",
    "\n",
    "# Calculate the cost using\n",
    "# cost = tf.reduce_sum(tf.pow((y_labels - output), 2), name='cost')\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y_labels * tf.log(y_output)))\n",
    "\n",
    "# Train the model, where 0.0001 is the learning rate\n",
    "learning_rate = 0.0001\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y_output, 1), tf.argmax(y_labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Initialize session to train the model - Just want to see the graph structure\n",
    "with tf.Session() as sess:\n",
    "    # Initialize the variables - since our graph will be small, can initialize all at once\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Write the graph to a local directory - will do this with training later\n",
    "\n",
    "    file_writer = tf.summary.FileWriter('./logs/1', sess.graph)\n",
    "    # Go to command line and write: $ tensorboard --logdir logs/1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the fruits of our labor so far in the Tensorboard representation. As well, we can see that some of the operations that happen 'under the hood' for functions we used. You can dig in here if you're curious about what is happening in these areas of the graph. Notice how the arrows are directed. The arrows in tensorboard show dependencies, not how data moves around. This is most evident when we look at how the weights and bias are updated from gradient descent, with the arrows for both of those parameters pointing to the operation.\n",
    "\n",
    "Now, let's make this look a little nicer so that we have larger chunks that can be isolated. After all, we only have our inputs, weights, biases, a matrix multiplication, sigmoid transformation, loss function, accuracy output, and gradients to compute. So 8 things. Let's compress these so they look like the image of a logistic regression from earlier. This means we only have our inputs and weights/bias layer, a sigmoid, output layer, and updating layer. So 4 layers. Awfully close to the 3 things I said we would learn. :^)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Hyperparameters - just learning rate for now, but this is good practice\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clear Tensorboard graph for troubleshooting\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Number of features and number of outputs\n",
    "num_features = len(list(X))\n",
    "num_outputs = 1\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, num_features], name = 'inputs')\n",
    "with tf.name_scope('Linear_Output'):\n",
    "    W = tf.Variable(tf.truncated_normal([num_features, num_outputs],\n",
    "                                        stddev=0.1), name = 'W')\n",
    "    b = tf.Variable(tf.zeros([num_outputs]), name = 'bias')\n",
    "    linear_output = tf.add(tf.matmul(x, W, name='mat_mul'), b, name='linear_output')\n",
    "\n",
    "    \n",
    "with tf.name_scope('Logits'):\n",
    "    logits = tf.sigmoid(linear_output, name='logit')\n",
    "    \n",
    "\n",
    "y_labels = tf.placeholder(tf.float32, [None, num_outputs], name = 'actual_output')\n",
    "with tf.name_scope('Cost'):\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y_labels * tf.log(logits))) # reduction_indices=[1]\n",
    "    \n",
    "\n",
    "with tf.name_scope('Train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "\n",
    "with tf.name_scope('Accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_labels, 1), name='correct_prediction')\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "\n",
    "\n",
    "# Initialize session to train the model - Just want to see the graph structure\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    file_writer = tf.summary.FileWriter('./logs/3', sess.graph)\n",
    "    # Go to command line and write: $ tensorboard --logdir logs/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tensorboard to Monitor Your Model's Progress\n",
    "Let's use this to find out how we can monitor our model's progress. We have all of the graph pieces nicely in place, so now we just need to utilize tensorboard's monitoring tools to understand how our model trains and updates weights. I'll introduce some more machine learning jargon, specifically the concept of an epoch. An epoch is one full forward and backward pass of the complete dataset. In our case, that means all of the rows in the imported csv files. Usually the files are much bigger than this one, which would require batching of the input data, which comes with a host of its own considerations. However, that's for later.\n",
    "\n",
    "Here, we want to progress how the model is updating the weights and bias as it trains. This is helpful in understanding where we can introduce some efficiencies to speed up training. I'll compare two different ways to initialize weights in this case.\n",
    "\n",
    "To do this comparison, we need to use another special kind of Tensorflow object, which is a Summary. A tf.summary class outputs a protocol buffer that contains the summarized data. In this case, we just want to see how weights/bias are changing, and how the accuracy is (hopefully) increasing. We'll look at tf.summary.scalar and tf.summary.histogram summaries to inspect the accuracy and weights/bias, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.0001\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clear Tensorboard graph for troubleshooting\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Number of features and number of outputs\n",
    "num_features = len(list(X))\n",
    "num_outputs = 1\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, num_features], name = 'inputs')\n",
    "\n",
    "with tf.name_scope('Linear_Output'):\n",
    "    W = tf.Variable(tf.truncated_normal([num_features, num_outputs],\n",
    "                                        stddev=0.1), name = 'W')\n",
    "    b = tf.Variable(tf.zeros([num_outputs]), name = 'bias')\n",
    "    linear_output = tf.add(tf.matmul(x, tf.cast(W, tf.float32), name='mat_mul'), b, name='linear_output')\n",
    "    tf.summary.histogram('Weights', W)\n",
    "    tf.summary.scalar('Bias', b)    \n",
    "\n",
    "    \n",
    "with tf.name_scope('Logits'):\n",
    "    logits = tf.sigmoid(linear_output, name='logit')\n",
    "    \n",
    "\n",
    "y_labels = tf.placeholder(tf.float32, [None, num_outputs], name = 'y_labels')\n",
    "with tf.name_scope('Cost'):\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y_labels * tf.log(logits)))\n",
    "    tf.summary.scalar('Cost', cost)\n",
    "    \n",
    "\n",
    "with tf.name_scope('Train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "\n",
    "# with tf.name_scope('Accuracy'):\n",
    "#     correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_labels, 1), name='correct_prediction')\n",
    "#     accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "#     tf.summary.scalar('Accuracy', accuracy)\n",
    "\n",
    "# Merge all of the summary statistics for convenience\n",
    "# This helps with MUCH larger models\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# Finally, time to train and test the model\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Since we now have training and testing data\n",
    "    file_writer = tf.summary.FileWriter('./logs/4', sess.graph)\n",
    "    # Go to command line and write: $ tensorboard --logdir logs/4\n",
    "    file_writer.add_graph(sess.graph)\n",
    "    for e in range(epochs):\n",
    "#         s = sess.run(merged, feed_dict={x: X.as_matrix(), y_labels:Y})\n",
    "#         file_writer.add_summary(s,e)\n",
    "        sess.run(optimizer, feed_dict={x: X.as_matrix(), y_labels:Y})\n",
    "        # Add an epoch counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29986, 1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Model with Regularization\n",
    "To do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
